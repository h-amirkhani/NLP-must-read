### BERT (2018)
- [BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding](https://arxiv.org/abs/1810.04805). Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova, 2018.
- Developed by Google AI Language and creates state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications.
- Useful resources:
   - [Official implementation](https://github.com/google-research/bert)

### UncoVec (2018)
- [Uncovering divergent linguistic information in word embeddings with lessons for intrinsic and extrinsic evaluation](http://aclweb.org/anthology/K18-1028) . Mikel Artetxe, Gorka Labaka, Inigo Lopez-Gazpio, Eneko Agirre . CoNLL, 2018.
- The winner of CoNLL 2018 best paper award.
- It investigates different linguistic information encoded in word vectors by post-processing them through a linear transformation that adjusts the similarity orders. 
- Useful resources:
  - [Official implementation](https://github.com/artetxem/uncovec)

### ELMo (2018)
- [Deep contextualized word representations](https://arxiv.org/pdf/1802.05365). Matthew E. Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark, Kenton Lee, Luke Zettlemoyer. NAACL, 2018. 
- **Context-based** word representation developed by AllenNLP using a deep bidirectional language model.
- Useful resources:
  - [ELMo in TensorFlow Hub](https://tfhub.dev/google/elmo/2)
  - [ELMo in Wolfram Cloud](https://resources.wolframcloud.com/NeuralNetRepository/resources/ELMo-Contextual-Word-Representations-Trained-on-1B-Word-Benchmark)
 
